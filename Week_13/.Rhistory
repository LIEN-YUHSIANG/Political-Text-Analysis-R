setwd("/Users/TIZONA/Desktop/Political-Text-Analysis-R/Week_13")
library(tidyverse)
library(readtext)
library(quanteda)
library(caret)
library(naivebayes)
library(e1071)
if(!require("kernlab")) {install.packages("kernlab"); library(kernlab)}
#-----------------------------------------------------------------------------#
# 0.0   Load Some Data: Trump Tweets
# Last week we looked at Biden-related tweets, and then you trained a model
# to classify Trump-related tweets in class. Let's work on the Trump tweets
# to begin with this week.
trump_tweets <- readtext("trump_tweets.csv", text_field = "text")
trump_dfm <- trump_tweets %>%
mutate(text = gsub("â€™", "'", .$text)) %>%
corpus() %>%
tokens(remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
split_hyphens = TRUE,
remove_separators = TRUE,
remove_url = TRUE) %>%
tokens_remove(stopwords(language = "en")) %>%
tokens_remove(c("0*", "@*")) %>%
tokens_wordstem(language = "en") %>%
dfm() %>%
dfm_remove(min_nchar=2) %>%
dfm_trim(min_docfreq = 5)
trump_dfm[ ntoken(trump_dfm) == 0, ]
# Two documents have zero tokens, so we have to drop them:
trump_dfm <- trump_dfm[ !ntoken(trump_dfm) == 0, ]
# Also remember that the dependent variable - the category we are trying to
# classify - should be explicitly set as a Factor variable, so that R knows it's
# a category label with a set of possible values (and not a String variable
# which could contain any word at all).
trump_dfm$label <- as.factor(trump_dfm@docvars$label)
trump_dfm$numeric_id <- 1:nrow(trump_dfm)
# Then partition the data randomly, preserving the label proportions...
set.seed(1234)
trainIndex <- createDataPartition(trump_dfm$label,
p = .8,
list = TRUE,
times = 1)
trump_dfm.train <- dfm_subset(trump_dfm, trump_dfm$numeric_id %in% trainIndex$Resample1 )
trump_dfm.test <- dfm_subset(trump_dfm, !trump_dfm$numeric_id %in% trainIndex$Resample1 )
# Finally, convert them into the matrix formats we use for training classifiers.
trump.train <- as.matrix(trump_dfm.train)
trump.test <- as.matrix(trump_dfm.test)
system.time({
set.seed(1234)
trump.svm <- svm(x = trump.train,
y = trump_dfm.train$label)
})
trump_svm.predict <- predict(trump.svm,
newdata = trump.test)
confusionMatrix(trump_svm.predict,
trump_dfm.test$label)
system.time({
svm.tune <- tune.svm(x = trump.train,
y = trump_dfm.train$label,
gamma = c(0.05, 0.1, 0.5, 1, 2),
cost = 10^(0:3))
})
getModelInfo("svmRadial")[[1]]$parameters
getModelInfo("svmRadial")[[1]]$grid
svm_grid <- expand.grid(sigma = c(1, 10, 100, 1000),
C = c(0.1, 1, 10)
)
svm_grid
svm_trCtrl <- trainControl(method = "cv",
number = 5)
system.time({
set.seed(1234)
trump_svm_c1 <- train(x = trump.train,
y = trump_dfm.train$label,
method = "svmRadial",
tuneGrid = svm_grid,
trControl = svm_trCtrl
)
})
trump_svm_c1
trump_svm_c1.predict <- predict(trump_svm_c1$finalModel,
newdata = trump.test)
confusionMatrix(trump_svm_c1.predict,
trump_dfm.test$label)
parallel::detectCores()
# This is the maximum number you can tell R to use. However, you probably
# shouldn't use the total number! It's better to leave a couple of cores free
# so that you can do other stuff on your computer while the models are being
# trained, especially if that process is likely to take a long time.
# My laptop has 12 cores, so I'm going to use 10 of them for training models.
# Set this to an appropriate level for your own computer.
numCores = 10
# Now you can set up your system for parallel processing with this command:
doParallel::registerDoParallel(cores = numCores)
View(trump_svm_c1)

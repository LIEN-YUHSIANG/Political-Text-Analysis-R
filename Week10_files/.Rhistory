iuyiuyi
q()
321+321
2137182378*13123
q()
q()
gotwd()
getwd()
load("~/Desktop/Quantitative Analysis R/qa_class5_files/japan_pop_edited.RData")
View(japan.pop)
View(japan_pop_edited)
load("~/Desktop/Quantitative Analysis R/qa_class5_files/japan_pop_edited.RData")
load("~/Desktop/Quantitative Analysis R/qa_class5_files/japan_pop_edited.RData")
View(japan_pop_edited)
setwd(qa_class5_files)
getwd()
setwd(qa_class5_files)
setwd("qa_class5_files")
getwd
getwd()
setwd("qa_class5_files")
setwd("Quantitative Analysis R")
getwd()
ls()
setwd("clear(")
clear()
q()
setwd("/Users/TIZONA/Desktop/Political Text Analysis R/Week10_files")
Sys.setlocale("LC_ALL", 'en_GB.UTF-8')
Sys.setenv(LANG = "en_GB.UTF-8")
# As ever, we'll use Tidyverse, readtext, and Quanteda:
library(tidyverse)
library(readtext)
library(quanteda)
# We're also using a new library this week, `stm`
if(!require("stm")) {install.packages("stm"); library(stm)}
news <- readtext("guardian_21.csv", text_field = "Text")
news
View(news)
news$Month <- month(news$Date)
news$Day <- day(news$Date)
news$text <- gsub("â€™", "'", news$text)
news_dfm <- news %>%
corpus() %>%
tokens(remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
split_hyphens = TRUE,
remove_separators = TRUE) %>%
tokens_remove(stopwords(language = "en")) %>%
tokens_wordstem(language = "en") %>%
dfm() %>%
dfm_remove(min_nchar=2)
news_dfm
news_dfm_trim <- dfm_trim(news_dfm, min_docfreq = 20)
news_dfm_trim
news_dfm_trim[ ntoken(news_dfm_trim) == 0, ]
# Here we see that 33 documents are now entirely blank. We have to remove those.
news_dfm_trim <- news_dfm_trim[ ntoken(news_dfm_trim) > 0, ]
news_dfm_trim
head(docvars(news_dfm_trim))
news_stm_dfm <- convert(news_dfm_trim,
to = "stm",
docvars( news_dfm_trim, c("Section", "Month", "Headline") )
)
set.seed(123)
news_stm_basic <- stm(news_stm_dfm$documents,
news_stm_dfm$vocab,
K = 15,
max.em.its = 100)
labelTopics( news_stm_basic, n = 8)
set.seed(123)
news_stm_basic <- stm(news_stm_dfm$documents,
news_stm_dfm$vocab,
K = 15,
max.em.its = 100)
labelTopics( news_stm_basic, n = 8)
plot.STM(news_stm_basic, type = "summary", labeltype = c("frex"), n=5)
plot.STM(news_stm_basic, type = "hist", labeltype = c("frex"))
# 2.2   Naming Topics
newsBasic_names <- c("Heealthcare",
"Gender",
"Calture",
"Afganistan",
"Natrue Disaster",
"UK Opposition",
"Covid-19",
"SNS",
"Nature",
"Crime",
"Us Politics",
"Energy",
"UK Gov",
"Cost of living",
"Supply Chains")
plot.STM(news_stm_basic, type = "summary",
topic.names = newsBasic_names, custom.labels = "")
news_stm_basic_corr <- topicCorr(news_stm_basic)
plot.topicCorr(news_stm_basic_corr, vlabels = newsBasic_names)
set.seed(456)
news_stm <- stm(news_stm_dfm$documents,
news_stm_dfm$vocab,
K = 15,
prevalence =~ Section + s(Month),
data = news_stm_dfm$meta,
max.em.its = 100)
labelTopics( news_stm, n = 6)
plot.STM(news_stm_basic, type = "summary", labeltype = c("frex"), n=5)
newsStruc_names <- c("Health Care",
"Medical Research",
"Calture",
"Afganistan",
"Natural Disaster",
"UK Opposition",
"Covid-19",
"SNS",
"Nature",
"Policing",
"US Politics",
"Energy",
"UK Gov",
"Cost of living",
"Supply Chain")
news_stm_corr <- topicCorr(news_stm)
plot.topicCorr(news_stm_corr, vlabels = newsStruc_names)
findThoughts(news_stm, texts = news_stm_dfm$meta$Headline, n = 3, topics = 1:15)
monthEffects <- estimateEffect( 1:15 ~ s(Month),
news_stm,
meta = news_stm_dfm$meta,
uncertainty = "Global" )
plot(monthEffects, "Month",
method = "continuous", topics = 4,
model = news_stm,
printlegend = FALSE)
plot(monthEffects, "Month",
method = "continuous", topics = 7,
model = news_stm,
printlegend = FALSE)
sectionEffects <- estimateEffect( 1:15 ~ Section,
news_stm,
meta = news_stm_dfm$meta,
uncertainty = "Global" )
unique(news_stm_dfm$meta$Section)
plot(sectionEffects, covariate = "Section", topics = 1:15,
model = news_stm, method = "difference",
cov.value1 = "News", cov.value2 = "Business",
xlab = "More Business ... More News",
main = "Effect of News vs. Business sections",
xlim = c(-.2, .2), labeltype = "custom",
custom.labels = newsStruc_names)
